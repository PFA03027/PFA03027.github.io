## Welcome to My Pages

私のページへようこそ
このページは、私の学びの記録としてのページです。

テーマは、「深層強化学習」
ChainerとChainerrlを使って、強化学習の実験環境の作成と、実験環境を使ってのエージェントの作成を行っています。

[My reinforcement learning agent and environment](https://github.com/PFA03027/twn_rl)

## 強化学習における環境
自作環境です。OpenGymとかを知る前に作成を進めていたため、自作臭がぬぐえません。

### エージェントが制御する対象
- 左右にそれぞれ１つの車輪が付いた乗り物

### 環境での目標
- エネルギーボールを取れたら成功。これが目標

### 環境から得られる情報
- 障害物との測距のための信号。反射信号の強度をそのまま入力信号とする。信号は全周に等角度で180本。
- 目標となるエネルギーボールへの方向と距離
- 制御対象となる乗り物の、左右の車輪の回転速度、自身の持っているエネルギーの残量

## エピソードの構成
- エネルギーボールを取得できるか、自身の持っているエネルギーの残量がゼロになったら終了。
- エネルギーボールを取得できた場合は、プラス報酬で終了
- 自身の持っているエネルギーの残量がゼロの場合は、マイナス報酬で終了

## エージェントのタイプ
まずは、DoubleDQNでエージェントを作成。なかなかグルグルから抜け出せず。
測距信号を処理する層(2層CNN＋全層結合1層の構成)を強化学習対象から外すことで、ようやく学習できるようになりました。

ちなみに、強化学習対象から外した測距信号を処理する層(2層CNN＋全層結合1層の構成)は、AutoEncoderで学習させました。

### 強化学習層の構成検討
#### アクション空間それぞれの学習層を持つ構成(Type3)での実験
Type2よりもうまく学習（少ないエピソード数で目標達成）。アクションのQ値学習の独立性を高める構成には、意味があるといえる。
今後は、Type3をベースに拡張していく。

### Expence Replayの方式検討
#### これまで
- Experience Replayには、Chainerrlのreplay bufferの実装をもとに、成功体験、失敗体験に相当する経験を優先保持する自作bufferを使用。

#### 今回 - アクション空間公正サンプリング方式
Experience Replayからの学習データサンプリングをアクション空間の方向で公正（各アクション同じ数）に学習データをサンプリングする方式を導入。
アクション空間を大きくしても、学習速度の低下を軽減。
学習データのサンプリングにおいて、アクション空間方向での偏りを軽減できたためと思われる。
- 離散空間で、9アクションから、25アクションに拡大

## エージェントの実装について
- Windows 10環境で、Anaconda3環境を構築。その上で、Chainer/Chainerrlを使用
- 強化学習を担う層は、ChainerrlのDoubleDQNを使用。
- 測距信号を処理する層は、ChainerのCNNを使用。CNN+reluの後にDropoutを挿入。Dropoutの出力をDeconvolutionし、AutoEncoderとしての学習を実施。
- CNN間の接続にPooling層を挿入する構成です。


